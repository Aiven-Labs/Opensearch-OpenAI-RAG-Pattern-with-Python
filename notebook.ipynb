{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic search using OpenSearch and OpenAI\n",
    "\n",
    "This notebook guides you through an example of using [Aiven for OpenSearch](https://go.aiven.io/openai-opensearch-os) as a backend vector database for OpenAI embeddings and how to perform semantic search.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you begin, ensure you have created all necessary accounts and services as highlighted in the [README](./README.md) to follow the prerequisites:\n",
    "- You have an [Aiven Account](./README.md#setup-your-aiven-account)\n",
    "- You have created your [opensearch service](./README.md#create-an-opensearch-service)\n",
    "- You have and OpenAI Account\n",
    "- You have created AND SAVED an OpenAI API key\n",
    "- You have setup your python environment for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding our Environment Variables\n",
    "To avoid leaking api_keys we will store them in an .env file that is ignored from version control.\n",
    "\n",
    "**make a copy of `.env_sample`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "! cp .env_sample .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Add our OpenAI API key\n",
    "\n",
    "Open `.env` and replace `<YOUR_OPENAI_API_KEY>` with the key that you saved from OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add our OpenSearch Service URI\n",
    "\n",
    "Verify the Aiven for OpenSearch service is in the `RUNNING` state.\n",
    "\n",
    "![OpenSearch service in the running state](./assets/opensearch-running-state.png)\n",
    "\n",
    "Select the running service and copy the **Service URI**.\n",
    "\n",
    "![Copy the OpenSearch Service URI](assets/copy-opensearch-service-uri.png)\n",
    "\n",
    "Add the OpenSearch Service URI to your `.env` file created above, replacing `<OPENSEARCH_SERVICE_URI>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os # to access our variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Connect to our Opensearch Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "\n",
    "connection_string = os.getenv(\"OPENSEARCH_SERVICE_URI\")\n",
    "\n",
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(connection_string, use_ssl=True, timeout=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset\n",
    "To save us from having to recalculate embeddings on a huge dataset, we are using a pre-calculated OpenAI embeddings dataset covering wikipedia articles. We can get the file and unzip it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import zipfile\n",
    "\n",
    "embeddings_url = 'https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip'\n",
    "wget.download(embeddings_url)\n",
    "\n",
    "with zipfile.ZipFile(\"vector_database_wikipedia_articles_embedded.zip\",\n",
    "\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the file in a dataframe and check the content with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wikipedia_dataframe = pd.read_csv(\"data/vector_database_wikipedia_articles_embedded.csv\")\n",
    "\n",
    "wikipedia_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains:\n",
    "* `id` a unique Wikipedia article identifier\n",
    "* `url` the Wikipedia article URL\n",
    "* `title` the title of the Wikipedia page\n",
    "* `text` the text of the article\n",
    "* `title_vector` and `content_vector` the embedding calculated on the title and content of the wikipedia article respectively\n",
    "* `vector_id` the id of the vector\n",
    "\n",
    "We can create an OpenSearch mapping optimized for this information with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings ={\n",
    "    \"index\": {\n",
    "      \"knn\": True,\n",
    "      \"knn.algo_param.ef_search\": 100\n",
    "    }\n",
    "  }\n",
    "\n",
    "index_mapping= {\n",
    "    \"properties\": {\n",
    "      \"title_vector\": {\n",
    "          \"type\": \"knn_vector\",\n",
    "          \"dimension\": 1536,\n",
    "          \"method\": {\n",
    "            \"name\": \"hnsw\",\n",
    "            \"space_type\": \"l2\",\n",
    "            \"engine\": \"faiss\"\n",
    "        }\n",
    "      },\n",
    "      \"content_vector\": {\n",
    "          \"type\": \"knn_vector\",\n",
    "          \"dimension\": 1536,\n",
    "          \"method\": {\n",
    "            \"name\": \"hnsw\",\n",
    "            \"space_type\": \"l2\",\n",
    "            \"engine\": \"faiss\"\n",
    "        },\n",
    "      },\n",
    "      \"text\": {\"type\": \"text\"},\n",
    "      \"title\": {\"type\": \"text\"},\n",
    "      \"url\": { \"type\": \"keyword\"},\n",
    "      \"vector_id\": {\"type\": \"long\"}\n",
    "      \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an index in Aiven for OpenSearch\n",
    "\n",
    "This is where we will store our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"openai_wikipedia_index\"\n",
    "client.indices.create(index=index_name, body={\"settings\": index_settings, \"mappings\":index_mapping})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index data into OpenSearch\n",
    "\n",
    "Now it's time to parse the the pandas dataframe and index the data into OpenSearch using Bulk APIs. The following function indexes a set of rows in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_bulk_actions(df):\n",
    "    for index, row in df.iterrows():\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": row['id'],\n",
    "            \"_source\": {\n",
    "                'url' : row[\"url\"],\n",
    "                'title' : row[\"title\"],\n",
    "                'text' : row[\"text\"],\n",
    "                'title_vector' : json.loads(row[\"title_vector\"]),\n",
    "                'content_vector' : json.loads(row[\"content_vector\"]),\n",
    "                'vector_id' : row[\"vector_id\"]\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to index all the dataset at once, since it's way too large, so we'll load it in batches of `200` rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import helpers\n",
    "import json\n",
    "\n",
    "start = 0\n",
    "end = len(wikipedia_dataframe)\n",
    "batch_size = 200\n",
    "for batch_start in range(start, end, batch_size):\n",
    "    batch_end = min(batch_start + batch_size, end)\n",
    "    batch_dataframe = wikipedia_dataframe.iloc[batch_start:batch_end]\n",
    "    actions = dataframe_to_bulk_actions(batch_dataframe)\n",
    "    helpers.bulk(client, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Verify that our index has populated in our Aiven Console\n",
    "\n",
    "In the Aiven Console, select **Indexes** in the sidebar and verify that you have documents populated. There should be OVER 20,000 documents.\n",
    "\n",
    "![OpenSearch Indexes in the Aiven Console](assets/opensearch-indexes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the documents are indexed, let's try a query to retrieve the documents containing `Pizza`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = client.search(index=index_name, body={\n",
    "    \"_source\": {\n",
    "        \"excludes\": [\"title_vector\", \"content_vector\"]\n",
    "    },\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"text\": {\n",
    "                \"query\": \"Pizza\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "print(res[\"hits\"][\"hits\"][0][\"_source\"][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode questions with OpenAI\n",
    "\n",
    "To perform a semantic search, we need to calculate questions encodings with the same embedding model used to encode the documents at index time. In this example, we need to use the `text-embedding-3-small` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Define model\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# Define the Client\n",
    "openaiclient = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "# Define question\n",
    "question = 'is Pineapple a good ingredient for Pizza?'\n",
    "\n",
    "# Create embedding\n",
    "question_embedding = openaiclient.embeddings.create(input=question, model=EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run semantic search queries with OpenSearch\n",
    "\n",
    "With the above embedding calculated, we can now run semantic searches against the OpenSearch index. We're using `knn` as query type and scan the content of the `content_vector` field.\n",
    "\n",
    "After running the block below, we should see content semantically similar to the question. Expect documents based on Pineapples, Pizza, Hawaii, Italy, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearch_response = client.search(\n",
    "  index = index_name,\n",
    "  body = {\n",
    "      \"size\": 15,\n",
    "      \"query\" : {\n",
    "        \"knn\" : {\n",
    "          \"content_vector\":{\n",
    "          \"vector\":  question_embedding.data[0].embedding,\n",
    "          \"k\": 3\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    ")\n",
    "\n",
    "for result in opensearch_response[\"hits\"][\"hits\"]:\n",
    "  print(\"Id:\" + str(result['_id']))\n",
    "  print(\"Score: \" + str(result[\"_score\"]))\n",
    "  print(\"Title: \" + str(result[\"_source\"][\"title\"]))\n",
    "  print(\"Text: \" + result[\"_source\"][\"text\"][0:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenAI Chat Completions API to generate a reply\n",
    "\n",
    "now let's use OpenAI chat `completions` to generate a reply based on the information retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the text of the first result in the above dataset\n",
    "top_hit_summary = opensearch_response['hits']['hits'][0]['_source']['text']\n",
    "\n",
    "# Craft a reply\n",
    "openai_response = openaiclient.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Answer the following question:\" \n",
    "            + question \n",
    "            + \"by using the following text:\" \n",
    "            + top_hit_summary\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "choices = openai_response.choices\n",
    "print(f\"Our top hit is \\n {top_hit_summary}\")\n",
    "for choice in choices:\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(choice.message.content)\n",
    "    print(\"------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "OpenSearch is a powerful tool providing both text and vector search capabilities. Used alongside OpenAI APIs allows you to craft personalized AI applications able to augment the context based on semantic search.\n",
    "\n",
    "You can try Aiven for OpenSearch, or any of the other Open Source tools, in the Aiven platform free trial by [signing up](https://go.aiven.io/openai-opensearch-signup)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
